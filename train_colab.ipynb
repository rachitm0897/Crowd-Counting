{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crowd_analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tempdata73/crowd_analysis/blob/master/train_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imABng0dsrwP",
        "colab_type": "text"
      },
      "source": [
        "# CSRNet: Congested Scene Recognition Network\n",
        "The purpose of this notebook is to train a CSRNet with the help of GPU instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNdQNhT2xRhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyDrive -qq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ5dZqsqZGN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/tempdata73/crowd_analysis.git\n",
        "%cd crowd_analysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha9l1tnSxkCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ6-XWCrx2-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoeThDOYyQ_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_ID = '16dhJn7k4FWVwByRsQAEpl9lwjuV03jVI'\n",
        "FILENAME = 'shanghai_dataset.zip'\n",
        "DST_DIR = 'Shanghai'\n",
        "\n",
        "download = drive.CreateFile({'id': FILE_ID})\n",
        "download.GetContentFile(FILENAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKDBHQW3yQ2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "ZipFile(FILENAME).extractall(DST_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm4BY3gRXZuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm shanghai_dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC3OGuvAcLUa",
        "colab_type": "text"
      },
      "source": [
        "## Density map generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IFILD5JcLDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "!python make_dataset.py Shanghai/ Shanghai_A\n",
        "end_time = time.time()\n",
        "print('Density map generation took {:0.2f} secs'.format(end_time - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er_o7vyiXODE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python train.py part_A/train_val.json part_A/test.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWtG-sopZcS4",
        "colab_type": "text"
      },
      "source": [
        "## Model tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4fQJgI0ZUTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "CKPT_DIR = 'ckpts'\n",
        "models = []\n",
        "\n",
        "for model_fn in os.listdir(CKPT_DIR):\n",
        "  if model_fn == 'model.pth.tar':\n",
        "    continue\n",
        "  model_path = os.path.join(CKPT_DIR, model_fn)\n",
        "  ckpt = torch.load(model_path, map_location=torch.device(device))\n",
        "  models.append((model_path, ckpt['loss']))\n",
        "\n",
        "models = sorted(models, key=lambda loss: loss[1], reverse=False)\n",
        "model_path = models[0][0]\n",
        "print(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAs71HOhKiiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python validation.py $model_path part_A/test.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QXfW5lSr_V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('data/metrics.json') as infile:\n",
        "  data = json.load(infile)\n",
        "\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etscb0CHAIxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Last epoch did not necessarily yield best model\n",
        "# Fix data according to that\n",
        "\n",
        "ckpt = torch.load(model_path, map_location=torch.device(device))\n",
        "last_epoch = ckpt['epoch']\n",
        "\n",
        "with open('data/loss_data.json') as infile:\n",
        "  loss = json.load(infile)\n",
        "\n",
        "last_ckpt_loss_data = {'train_mae': loss['train_mae'][:last_epoch], 'val_mae': loss['val_mae'][:last_epoch]}\n",
        "\n",
        "with open('data/loss_data.json', 'w') as outfile:\n",
        "  json.dump(last_ckpt_loss_data, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fok2K6zE1KwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download model and data\n",
        "from google.colab import files\n",
        "\n",
        "files.download(model_path)\n",
        "files.download('data/metrics.json')\n",
        "files.download('data/loss_data.json')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}